from typing import Optional, Dict, List, Set, Deque
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import deque
import time
import random

class InternalWebsiteCrawler:
    def __init__(self, base_url: str, max_pages: int = 100, delay: float = 1.0, timeout: int = 10):
        self.base_url = base_url
        self.base_domain = urlparse(base_url).netloc
        self.max_pages = max_pages
        self.delay = delay
        self.timeout = timeout
        
        
        self.visited_urls: Set[str] = set()
        self.internal_urls: List[str] = []  
        self.queue: Deque[str] = deque()
        
        
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept-Language": "ru-RU,ru;q=0.9",
            "Referer": "https://www.google.com/"
        })

    def _normalize_url(self, url: str) -> str:
      
        parsed = urlparse(url)
        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/').lower()
        return clean_url

    def _is_valid_internal_url(self, url: str) -> bool:
      
        parsed = urlparse(url)
        
        
        if not parsed.scheme or not parsed.netloc:
            return False
        if parsed.scheme not in ('http', 'https'):
            return False
        if parsed.netloc != self.base_domain:
            return False
            
        
        if any(parsed.path.lower().endswith(ext) for ext in 
             ['.png', '.jpg', '.jpeg', '.gif', '.pdf', '.doc', '.docx', '.xls', '.xlsx']):
            return False
            
        return True

    def _should_skip_url(self, url: str) -> bool:
      
        skip_patterns = {
            '#', 'javascript:', 'mailto:', 'tel:', 'data:',
            'login', 'signup', 'logout', 'wp-admin', 'admin',
            'feed', 'rss', '.xml', '.json', 'api/'
        }
        url_lower = url.lower()
        return any(pattern in url_lower for pattern in skip_patterns)

    def _fetch_url(self, url: str) -> Optional[requests.Response]:
        
        try:
        
            time.sleep(random.uniform(self.delay/2, self.delay*1.5))
            
            response = self.session.get(
                url,
                timeout=self.timeout,
                allow_redirects=True
            )
            response.raise_for_status()
            
         
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' not in content_type:
                print(f"Пропускаем (не HTML): {url} - {content_type}")
                return None
                
            return response
            
        except requests.exceptions.RequestException as e:
            print(f"Ошибка при запросе {url}: {str(e)}")
            return None

    def _process_page(self, url: str) -> None:
    
        normalized_url = self._normalize_url(url)
        
        if normalized_url in self.visited_urls:
            return
            
        print(f"Обработка [{len(self.visited_urls)+1}/{self.max_pages}]: {normalized_url}")
        self.visited_urls.add(normalized_url)
        
        response = self._fetch_url(normalized_url)
        if not response:
            return
            
       
        if normalized_url not in self.internal_urls:
            self.internal_urls.append(normalized_url)
        
       
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for link in soup.find_all('a', href=True):
            href = link['href'].strip()
            if not href or self._should_skip_url(href):
                continue
                
            absolute_url = urljoin(normalized_url, href)
            clean_url = self._normalize_url(absolute_url)
            
            if self._is_valid_internal_url(clean_url) and clean_url not in self.visited_urls:
                self.queue.append(clean_url)

    def crawl(self) -> Dict[str, List[str]]:
        
        self.queue.append(self.base_url)
        
        while self.queue and len(self.visited_urls) < self.max_pages:
            current_url = self.queue.popleft()
            self._process_page(current_url)
        
        print("\nСканирование завершено.")
        return {
            "internal_urls": self.internal_urls,
            "total_pages": len(self.visited_urls)
        }

    def save_to_file(self, filename: str = "internal_links.txt"):
      
        with open(filename, 'w', encoding='utf-8') as f:
            for url in self.internal_urls:
                f.write(url + '\n')
        print(f"\nСсылки сохранены в файл: {filename}")

if __name__ == "__main__":
    
    TARGET_URL = "https://sever-rb.orb.ru/"
    MAX_PAGES = 300
    DELAY = 0.5 
    TIMEOUT = 5
    
    print(f"Начало сканирования сайта: {TARGET_URL}")
    start_time = time.time()
    
    crawler = InternalWebsiteCrawler(
        base_url=TARGET_URL,
        max_pages=MAX_PAGES,
        delay=DELAY,
        timeout=TIMEOUT
    )
    
    result = crawler.crawl()
    
    crawler.save_to_file()
    
   
    duration = time.time() - start_time
    print(f"\nСтатистика:")
    print(f" - Всего страниц обработано: {result['total_pages']}")
    print(f" - Найдено ссылок: {len(result['internal_urls'])}")
    
    for i, url in enumerate(result['internal_urls'][:300], 1):
        print(f"{i:3}. {url}")
